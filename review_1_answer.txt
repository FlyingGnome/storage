Review answer:

Drawbacks and Responses:
Drawback 1: The abstract and introduction discuss divergent MobileNetV3 behavior, yet the results, tables, and figures presented are LCNet-only. This creates a mismatch between claims and evidence.
Response 1: Thank you for this insightful observation. We fully agree with the reviewer that the previous version created a mismatch between the abstract/introductory claims and the LCNet-only results presented in the manuscript.
To address this issue, we have revised the manuscript to ensure full consistency:
1.	We removed the statements referring to MobileNetV3 divergence from the Abstract, since this study focuses exclusively on LCNet backbones.
2.	We clarified in the ‘Literature Review and Problem Statement’ section that the MobileNetV3 vs. LCNet comparison was extensively conducted in our previous publication, and that the present work builds upon those findings while focusing solely on LCNet scaling behavior.
3.	We added an explicit sentence explaining why MobileNetV3 results are not included in this manuscript, ensuring no expectation mismatch for the reader.
These revisions can be found in:
Page 4, Literature Review and Problem Statement section, Paragraph 8, Lines 139-143

Drawback 2: Section 5 looks like Results, not the Methodology.
Response 2: Thank you for this helpful comment. We agree that the previous section title was misleading, as Section 5 contained experimental outcomes rather than methodological descriptions. 
To correct this and improve the logical structure of the manuscript, we have renamed Section 5 from “Methodology” to “Results”.
These revisions can be found in:
Page 10, Line 317

Drawback 3: If the hybrid introduces a 4-D bottleneck and an extra projection, the classical comparators should include parameter-matched or bottleneck-matched baselines.
Response 3: Thank you for this valuable suggestion. We fully agree that including parameter-matched or bottleneck-matched classical baselines would strengthen the analysis and provide a more controlled comparison between hybrid and classical models.
However, such an investigation requires developing additional baseline architectures and conducting a new set of experiments, which extends beyond the scope of the present study. This work focuses specifically on evaluating scaling effects within LCNet backbones when integrating a quantum layer, rather than performing architecture-level ablations.

Drawback 4: Means and standard deviations are reported across folds, but there are no significance tests or confidence intervals. Several CIFAR-10 deltas are ~0.5-1.0% and may be due to noise.
Response 4: Thank you for this important remark. We fully agree that many of the CIFAR-10 improvements fall within a narrow range and may not reach statistical significance. In the revised manuscript, we have explicitly clarified this point in the CIFAR-10 Results section.

Drawback 5: On DermaMNIST, accuracy/AUC are reported, but clinically relevant diagnostics are only mentioned, not shown. Given class imbalance and clinical framing, calibration and class-wise performance are important.
Response 5: Thank you very much for this constructive comment. We fully agree that for clinical applications, metrics such as class-wise performance and calibration curves are highly valuable.
However, in this study we intentionally followed the standard evaluation protocol of the MedMNIST benchmark, which is widely used in medical-imaging machine learning research. The DermaMNIST dataset was originally designed as a lightweight classification benchmark.
Because our goal was to provide a controlled backbone-scaling comparison within the established MedMNIST evaluation framework, we did not extend the analysis to include calibration curves or detailed per-class diagnostic sensitivity/specificity. We now emphasize this point clearly in the manuscript.

Drawback 6: The paper could better situate itself against recent HNN/QML for medical imaging and clarify nomenclature. Additionally, LCNet scales up to 150/200, which appears in the results but is not introduced earlier in the methods section.
Response 6: Thank you for this helpful suggestion. We fully agree that clearer contextualization and earlier introduction of LCNet150/200 improve the clarity and continuity of the manuscript.
To address this:
1.	We expanded the methodological description of LCNet in Section 4.5, explicitly introducing the LCNet150 and LCNet200 variants before they appear in the results.
2.	We clarified terminology and added additional contextual information to better situate our work within ongoing research on hybrid quantum-classical neural networks for medical imaging.    
These revisions can be found in:\
Page 1, Abstract, Mehods section, Line 9
Page 4, Methodology section, Paragraph 1, Lines 163-164

Recommendations and Responses:
Recommendation 1: Rename Section 5 to “Results”.
Response 1: Thank you for this helpful comment. We agree that the previous section title was misleading, as Section 5 contained experimental outcomes rather than methodological descriptions. 
To correct this and improve the logical structure of the manuscript, we have renamed Section 5 from “Methodology” to “Results”.
These revisions can be found in:
Page 10, Line 317

Recommendation 2: Add a compact schematic of each pipeline variant used. In each table, include a column stating which variant is being reported.
Response 2: Thank you for this excellent recommendation. We fully agree that visual schematics and explicit variant labeling improve the clarity and reproducibility of the manuscript.
To address this:
1.	We added a compact pipeline schematic that illustrates the classical and hybrid variants used in the study.
	•	This new figure appears in Section 4 (Methodology),  4.5 Classical Backbone subsection, page 9, lines 278–279.
2.	We updated all result tables to include a dedicated “Variant” column, explicitly indicating whether the reported configuration corresponds to:
	•	Classical baseline
	•	Hybrid model
This ensures that readers can easily identify which architecture variant each metric corresponds to.
These revisions can be found in:
Page 11, Table 1
Page 12, Table 2
Page 13, Table 3
Page 14, Table 4

Recommendation 3: Clarify in the text why MobileNetV3 results are not included.
Response 3: Thank you for this valuable recommendation. We fully agree that reporting 95% confidence intervals and performing paired significance testing would greatly strengthen the statistical reliability of the comparisons, especially for small deltas in the CIFAR-10 experiments.
However, this study was not originally planned with full fold-level archival of intermediate predictions and metrics. As a result:
1. Some of the required per-fold raw outputs are no longer available,
2. Which makes it impossible to compute confidence intervals and paired tests retrospectively for all backbone–scale combinations.
We now clearly acknowledge this limitation in the manuscript.
At the same time, we recognize the importance of this methodology for future research. Therefore we added a statement in the Discussion (Limitations and Future Research) that future experiments will include full fold-level logging
These revisions can be found in:
Page 17, Section 6.3 Limitations and Future Research, Paragraph 3, Lines 404-407


Recommendation 4: List all hyperparameters, number of trainable PQC parameters, seeds, and stopping criteria. Include hardware details and wall-clock training/inference times to contextualize practicality.
Response 4: Thank you for this valuable recommendation. We agree that clearly documenting all training and experimental settings substantially improves reproducibility and provides important context for interpreting the results.
To address this, we expanded the Workflow section by adding all available and relevant configuration details used throughout the study, including:
	•	the QC5 input configuration (3 RGB channels + 4 quantum-generated channels),
	•	6-fold cross-validation,
	•	batch size,
	•	optimizer type and learning rate,
	•	number of epochs,
	•	random seed settings,
	•	software version,
	•	and hardware environment.
We also clarified why direct wall-clock time comparisons between classical and hybrid models were not included, due to preprocessing overhead associated with generating quantum-enhanced channels, which is independent of the training loop and would bias the comparison.


We additionally incorporated references to prior research employing comparable hybrid quantum–classical configurations to further contextualize the methodological choices used in this study.
